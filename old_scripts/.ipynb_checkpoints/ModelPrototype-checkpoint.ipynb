{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012761e",
   "metadata": {},
   "source": [
    "# Model Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f1b35",
   "metadata": {},
   "source": [
    "### This notebook shows how to create a baseline model pipeline and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0aed4",
   "metadata": {},
   "source": [
    "##### We save the Spark Dataframe as an Iceberg Table. Iceberg is a new open table format backed by Apple, Netflix and Cloudera. \n",
    "##### In the context of ML Ops, the most anticipated feature is Time Travel i.e. the ability to reproduce the data and the schema across different versions in time\n",
    "##### Finally, we create a simple PySpark pipeline and train a classifier with Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1246c56",
   "metadata": {},
   "source": [
    "* For a more comprehensive demo of Iceberg in CML, please visit the [Spark3 Iceberg CML Github Repository](https://github.com/pdefusco/Spark3_Iceberg_CML)\n",
    "* For a more detailed introduction to CML Session, Notebooks, and Spark tips and trips please visit the [CML Total Beginner GitHub Repository](https://github.com/pdefusco/CML-Total-Beginner)\n",
    "* For a more comprehensive example of the Atlas Python client mentioned below, please visit the [Atlas Client Example Notebook in the Data Integration with ML GitHub Repository](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaee738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.datasets import make_circles\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from helpers.plot_decision_boundary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6efdef-6885-47de-96c2-c04340e5b622",
   "metadata": {},
   "source": [
    "#### The Spark Session is created with the following configurations. If you get an error, ensure your CML Session is using Runtimes and Spark 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef66ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08e0e-b080-4185-9771-650cfbf89425",
   "metadata": {},
   "source": [
    "#### Just some fake data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095723d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1      var2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)\n",
    "\n",
    "circles = pd.DataFrame({\"var1\":X[:, 0], \"var2\":X[:, 1], \"label\":y})\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40cd0-af1e-4f84-868f-00b0ffcb6a32",
   "metadata": {},
   "source": [
    "#### We can save the DataFrame as an Iceberg Table using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f2ca2e-1f94-4524-b206-8c838b207424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Dataframe from the Pandas Dataframe\n",
    "sparkDF=spark.createDataFrame(circles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77b4951-fb99-4ab7-b854-74c1cdcd6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (var1 int, var2 int, label int) USING iceberg\")\n",
    "\n",
    "sparkDF.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.ice_cml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc67509-f906-40fd-af65-87d5036673dc",
   "metadata": {},
   "source": [
    "#### The table is automatically tracked by the Data Lake associated with the CML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb1ce-e595-45c1-b080-40c3accbd0b6",
   "metadata": {},
   "source": [
    "#### To check that a new entry for the table has been added to Atlas in the Data Lake, go back to the CDP Homepage and open Data Catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bc771-6981-4472-a781-49d651144da8",
   "metadata": {},
   "source": [
    "#### Select the Data Lake (i.e. Cloud Environment) that your worskpace was built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015a62d-5ce3-492d-90d2-7ff9e51979b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50fc4b88-e6ac-4555-b680-a7bca4cee876",
   "metadata": {},
   "source": [
    "#### Use the Atlas Search bar at the top to browse for the table and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6429de-8a14-413f-9ee5-d8f8ce6ce603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f11ac-83a2-4ef8-b19b-cbc0b5595602",
   "metadata": {},
   "source": [
    "#### Notice Atlas is tracking a lot of interesting Metadata including Table Attributes, Lineage, and a lot More. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1b4f-41b2-4b8c-88ba-65b377eaf897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d11d0e-6d5b-4c00-9cd2-f14994758040",
   "metadata": {},
   "source": [
    "#### The Metadata can even be customized. [This notebook](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb) shows how you can use the Atlas Python Client to build custom lineage flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62128e78-ec09-4963-af8a-434f01497c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8dcc7b-1252-48f9-9d5f-10e9efb6bd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, y_train = X[:800], y[:800] # 80% of the data for the training set\n",
    "X_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n",
    "\n",
    "# Check the shapes of the data\n",
    "X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=25)\n",
    "\n",
    "# Evaluate our model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "# Plot the decision boundaries for the training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X=X_test, y=y_test)\n",
    "plt.show()\n",
    "\n",
    "# You can access the information in the history variable using the .history attribute\n",
    "pd.DataFrame(history.history)\n",
    "\n",
    "# Plot the loss curves\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.title(\"Model training curves\")\n",
    "\n",
    "\n",
    "model.save('models/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
