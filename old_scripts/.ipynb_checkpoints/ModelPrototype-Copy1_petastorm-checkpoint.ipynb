{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012761e",
   "metadata": {},
   "source": [
    "# Model Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f1b35",
   "metadata": {},
   "source": [
    "### This notebook shows how to create a baseline model pipeline and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0aed4",
   "metadata": {},
   "source": [
    "##### We save the Spark Dataframe as an Iceberg Table. Iceberg is a new open table format backed by Apple, Netflix and Cloudera. \n",
    "##### In the context of ML Ops, the most anticipated feature is Time Travel i.e. the ability to reproduce the data and the schema across different versions in time\n",
    "##### Finally, we create a simple PySpark pipeline and train a classifier with Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1246c56",
   "metadata": {},
   "source": [
    "* For a more comprehensive demo of Iceberg in CML, please visit the [Spark3 Iceberg CML Github Repository](https://github.com/pdefusco/Spark3_Iceberg_CML)\n",
    "* For a more detailed introduction to CML Session, Notebooks, and Spark tips and trips please visit the [CML Total Beginner GitHub Repository](https://github.com/pdefusco/CML-Total-Beginner)\n",
    "* For a more comprehensive example of the Atlas Python client mentioned below, please visit the [Atlas Client Example Notebook in the Data Integration with ML GitHub Repository](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaee738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.datasets import make_circles\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from helpers.plot_decision_boundary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54236788-fce6-4633-9f80-98516bc8fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Using cached petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n",
      "Collecting dill>=0.2.1\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting future>=0.10.2\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.7/site-packages (from petastorm) (19.0.2)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.6 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "Collecting diskcache>=3.0.0\n",
      "  Using cached diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
      "Requirement already satisfied: pandas>=0.19.0 in ./.local/lib/python3.7/site-packages (from petastorm) (1.3.4)\n",
      "Requirement already satisfied: packaging>=15.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (21.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from petastorm) (1.19.4)\n",
      "Collecting pyspark>=2.1.0\n",
      "  Using cached pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (1.16.0)\n",
      "Collecting psutil>=4.0.0\n",
      "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 109.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from packaging>=15.0->petastorm) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Using cached py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: future, pyspark\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=b38ac548f872c7df16598b5266fb85c07b824d62962b799c7f38e206e4b61fcb\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=a686bb367410b73802b915627544ea7673f707677505407d52ea395e55eeb0ce\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
      "Successfully built future pyspark\n",
      "Installing collected packages: py4j, pyspark, pyarrow, psutil, future, fsspec, diskcache, dill, petastorm\n",
      "Successfully installed dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 future-0.18.2 petastorm-0.11.3 psutil-5.8.0 py4j-0.10.9.2 pyarrow-6.0.1 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install petastorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6efdef-6885-47de-96c2-c04340e5b622",
   "metadata": {},
   "source": [
    "#### The Spark Session is created with the following configurations. If you get an error, ensure your CML Session is using Runtimes and Spark 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714803d8-bc39-428e-9ed9-73c2e9e4c485",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d797de-bc88-472d-a89e-72aeed7d75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def download_mnist_libsvm(mnist_data_dir):\n",
    "    mnist_data_path = os.path.join(mnist_data_dir, \"mnist.bz2\")\n",
    "    data_url = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2\"\n",
    "    r = requests.get(data_url)\n",
    "    with open(mnist_data_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "\n",
    "def get_mnist_dir():\n",
    "    # This folder is baked into the docker image\n",
    "    MNIST_DATA_DIR = \"/home/cdsw/data/mnist/\"\n",
    "\n",
    "    if os.path.isdir(MNIST_DATA_DIR) and os.path.isfile(os.path.join(MNIST_DATA_DIR, 'mnist.bz2')):\n",
    "        return MNIST_DATA_DIR\n",
    "\n",
    "    download_mnist_libsvm(MNIST_DATA_DIR)\n",
    "    return MNIST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bc3273-9155-47a0-9235-5128e5e7fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dir = get_mnist_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d973ab09-d1cf-42e8-9cac-ed08bf1c460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.7/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.functions import col\n",
    "except ImportError:\n",
    "    raise ImportError(\"This script runs with PySpark>=3.0.0\")\n",
    "\n",
    "\n",
    "def get_compiled_model(lr=0.001):\n",
    "    from tensorflow import keras\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(dataset, steps=1000, lr=0.001):\n",
    "    model = get_compiled_model(lr=lr)\n",
    "    model.fit(dataset, steps_per_epoch=steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "449f245d-f49d-42e0-a774-12b78422f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SparkSession\n",
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\")\\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\")\\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://gd01-uat2/\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd021bac-4740-42c4-b3a1-f5bf9311d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using Spark\n",
    "df = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(mnist_dir) \\\n",
    "    .select(col(\"features\"), col(\"label\").cast(\"long\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da7ac930-d23d-4af8-abc3-97c07ae671da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: bigint]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96f260-6009-4074-8121-09755fcceeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS new_ice (features ARRAY<INT>, label BIGINT) USING iceberg\")\n",
    "\n",
    "sparkDF.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.new_ice\")\n",
    "\n",
    "spark.read.format(\"iceberg\").load(\"default.new_ice.snapshots\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00334d59-dba4-4388-9de0-04e53749cc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87f21b-879d-46ec-869a-9b608eb31a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ecfcd52-2cb7-463a-ac4a-040be6b9a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"spark_catalog.default.mnist_iceberg_cml\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b24b3dbe-b437-4022-b026-a75936daa90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.mnist_iceberg_cml.snapshots not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2cf6ad119aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default.mnist_iceberg_cml.snapshots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table default.mnist_iceberg_cml.snapshots not found"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"default.mnist_iceberg_cml.snapshots\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00726986-3265-4cc4-be25-02ad8b86c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94425b6e-15d2-490b-8f03-6e26f956ea9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o387.showString.\n: java.io.IOException: Can't get Master Kerberos principal for use as renewer\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:81)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:217)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:442)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d524982fad37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from default.mnist_iceberg_cml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o387.showString.\n: java.io.IOException: Can't get Master Kerberos principal for use as renewer\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:81)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:217)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:442)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.mnist_iceberg_cml\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d80453-5082-4716-9b8f-43aed79ffaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ddbb1a7-d11d-41a2-b44c-0dcafb3fdb58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write incompatible data to table 'spark_catalog.default.mnist_iceberg':\n- Cannot write 'features': struct<type:tinyint,size:int,indices:array<int>,values:array<double>> is incompatible with string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c134dc08d4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE IF NOT EXISTS ice_cml (features string, label bigint) USING iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default.mnist_iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write incompatible data to table 'spark_catalog.default.mnist_iceberg':\n- Cannot write 'features': struct<type:tinyint,size:int,indices:array<int>,values:array<double>> is incompatible with string"
     ]
    }
   ],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (features string, label bigint) USING iceberg\")\n",
    "\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.mnist_iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516e0a96-a5ba-45a1-8449-8d4374d254e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a4745c-dfac-43d2-8055-8ea4d45c20ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.7/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "Converting floating-point columns to float32\n",
      "The median size 10765737 B (< 50 MB) of the parquet files is too small. Total size: 16658412 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211204003041-appid-local-1638577702563-7cae4a6d-21d7-49ea-aa1b-7163ea42d401/part-00001-036bb370-60b7-4c80-931f-f84d2c303dbd-c000.parquet, ...\n",
      "Converting floating-point columns to float32\n",
      "The median size 1231397 B (< 50 MB) of the parquet files is too small. Total size: 1903223 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211204003051-appid-local-1638577702563-dffa2649-1747-4b1a-8996-520243496975/part-00001-ae58eb09-37df-46fc-8aa2-997f1ee352ee-c000.parquet, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 1ms/step - loss: 4.4996 - accuracy: 0.6770\n",
      "193/193 [==============================] - 1s 3ms/step - loss: 1.2330 - accuracy: 0.6653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Randomly split data into train and test dataset\n",
    "df_train, df_test = df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "# Set a cache directory for intermediate data.\n",
    "# The path should be accessible by both Spark workers and driver.\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\n",
    "               \"file:///tmp/petastorm/cache/tf-example\")\n",
    "\n",
    "converter_train = make_spark_converter(df_train)\n",
    "converter_test = make_spark_converter(df_test)\n",
    "\n",
    "def train_and_evaluate(_=None):\n",
    "    import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n",
    "\n",
    "    with converter_train.make_tf_dataset() as dataset:\n",
    "        dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n",
    "        model = train(dataset)\n",
    "\n",
    "    with converter_test.make_tf_dataset(num_epochs=1) as dataset:\n",
    "        dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n",
    "        hist = model.evaluate(dataset)\n",
    "\n",
    "    return hist[1]\n",
    "\n",
    "# Train and evaluate the model on the local machine\n",
    "accuracy = train_and_evaluate()\n",
    "logging.info(\"Train and evaluate the model on the local machine.\")\n",
    "logging.info(\"Accuracy: %.6f\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e6971f-08f1-4372-b3b5-a92108aafd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6653121113777161"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9a457-a3f2-42a2-9503-596cfd429fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Train and evaluate the model remotely on a spark worker, \"\n",
    "             \"which can be used for distributed hyperparameter tuning.\")\n",
    "logging.info(\"Accuracy: %.6f\", accuracy)\n",
    "\n",
    "# Cleanup\n",
    "converter_train.delete()\n",
    "converter_test.delete()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f08e0e-b080-4185-9771-650cfbf89425",
   "metadata": {},
   "source": [
    "#### Just some fake data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095723d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1      var2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)\n",
    "\n",
    "circles = pd.DataFrame({\"var1\":X[:, 0], \"var2\":X[:, 1], \"label\":y})\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40cd0-af1e-4f84-868f-00b0ffcb6a32",
   "metadata": {},
   "source": [
    "#### We can save the DataFrame as an Iceberg Table using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f2ca2e-1f94-4524-b206-8c838b207424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Dataframe from the Pandas Dataframe\n",
    "sparkDF=spark.createDataFrame(circles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77b4951-fb99-4ab7-b854-74c1cdcd6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Spark Dataframe as an Iceberg table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS ice_cml (var1 int, var2 int, label int) USING iceberg\")\n",
    "\n",
    "sparkDF.write.format(\"iceberg\").mode(\"overwrite\").save(\"default.ice_cml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc67509-f906-40fd-af65-87d5036673dc",
   "metadata": {},
   "source": [
    "#### The table is automatically tracked by the Data Lake associated with the CML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb1ce-e595-45c1-b080-40c3accbd0b6",
   "metadata": {},
   "source": [
    "#### To check that a new entry for the table has been added to Atlas in the Data Lake, go back to the CDP Homepage and open Data Catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bc771-6981-4472-a781-49d651144da8",
   "metadata": {},
   "source": [
    "#### Select the Data Lake (i.e. Cloud Environment) that your worskpace was built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015a62d-5ce3-492d-90d2-7ff9e51979b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50fc4b88-e6ac-4555-b680-a7bca4cee876",
   "metadata": {},
   "source": [
    "#### Use the Atlas Search bar at the top to browse for the table and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6429de-8a14-413f-9ee5-d8f8ce6ce603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f11ac-83a2-4ef8-b19b-cbc0b5595602",
   "metadata": {},
   "source": [
    "#### Notice Atlas is tracking a lot of interesting Metadata including Table Attributes, Lineage, and a lot More. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1b4f-41b2-4b8c-88ba-65b377eaf897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d11d0e-6d5b-4c00-9cd2-f14994758040",
   "metadata": {},
   "source": [
    "#### The Metadata can even be customized. [This notebook](https://github.com/pdefusco/Data_Integration_wMachineLearning/blob/main/2_A_Atlas_Client_Example.ipynb) shows how you can use the Atlas Python Client to build custom lineage flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62128e78-ec09-4963-af8a-434f01497c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81838f58-3139-40d9-87d9-a824fc740440",
   "metadata": {},
   "source": [
    "#### Back to Modeling. We will use Keras and Tensorflow to build this classifier. Our data is in Spark though, so we will use Petastorm to transform the data Tensorflow-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f64479-85fd-40b6-980e-6eb594e49464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Downloading petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from petastorm) (1.19.4)\n",
      "Collecting psutil>=4.0.0\n",
      "  Downloading psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 92.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.19.0 in ./.local/lib/python3.6/site-packages (from petastorm) (1.1.5)\n",
      "Collecting future>=0.10.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 113.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting diskcache>=3.0.0\n",
      "  Downloading diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 451 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=15.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (21.0)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.6 MB 116.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.6/site-packages (from petastorm) (19.0.2)\n",
      "Collecting dill>=0.2.1\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 880 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pyspark>=2.1.0\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████    | 246.9 MB 96.6 MB/s eta 0:00:011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 281.3 MB 34 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from petastorm) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from packaging>=15.0->petastorm) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /runtime-addons/cmladdon-python-2.0.24-b100/opt/cmladdons/python/site-packages (from pandas>=0.19.0->petastorm) (2021.3)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 110.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, pyspark\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=32fda17f62ac6e87eaef8a6e235595e4c7024d506fdcc3aaba3a14d39688e0fc\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=1b9e3b29a52b66939ba7f5befa8ff94dbd56c39dcfbe7b8fc7924fe89509f65e\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/e8/d9/e5/78436a0a3899d81410aeb45b200153113667f2e250f6882ada\n",
      "Successfully built future pyspark\n",
      "Installing collected packages: py4j, pyspark, pyarrow, psutil, future, fsspec, diskcache, dill, petastorm\n",
      "Successfully installed dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 future-0.18.2 petastorm-0.11.3 psutil-5.8.0 py4j-0.10.9.2 pyarrow-6.0.1 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e3b0c9-4416-4848-9f22-f8e37b000978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "import tensorflow.compat.v1 as tf  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293756f5-2346-40a8-bbc1-35c994a9cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a cache dir first.\n",
    "\n",
    "# Set a cache directory for intermediate data.\n",
    "# The path should be accessible by both Spark workers and driver.\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\"file:///tmp/petastorm/cache/tf-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0c17f-7d10-4014-b93a-b7b5e3233502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f5b076-ee95-49fb-8105-5c41ead4a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting floating-point columns to float32\n",
      "The median size 5033 B (< 50 MB) of the parquet files is too small. Total size: 10062 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/cache/tf-example/20211203233933-appid-local-1638570042715-2c963542-b1cd-4cd6-8bf5-1297cbb97040/part-00000-db00b0de-8eba-40c6-8675-01b28dff976b-c000.parquet, ...\n"
     ]
    }
   ],
   "source": [
    "# create a converter from `df`\n",
    "# it will materialize `df` to cache dir.\n",
    "converter = make_spark_converter(sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7bd49-8ac7-4047-9a73-d9dd82dc4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff008b7-bd09-4555-a00b-f6aa5735dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017d7c69-e414-4946-b212-01bf6a5afb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d548d6e-cdd3-4573-8937-7b0a4a8a41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 607779/Unknown - 524s 860us/step - loss: -900.7520 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# make a tensorflow dataset from `converter`\n",
    "with converter.make_tf_dataset() as dataset:\n",
    "    # the `dataset` is `tf.data.Dataset` object\n",
    "    # we can train/evaluate model on the `dataset`\n",
    "    history = model.fit(dataset)\n",
    "    # when exiting the context, the reader of the dataset will be closed\n",
    "    \n",
    "    # Evaluate our model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Model loss on the test set: {loss}\")\n",
    "    print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# delete the cached files of the dataframe.\n",
    "converter.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, y_train = X[:800], y[:800] # 80% of the data for the training set\n",
    "X_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n",
    "\n",
    "# Check the shapes of the data\n",
    "X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model (same as model_7)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n",
    "  tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=25)\n",
    "\n",
    "# Evaluate our model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n",
    "\n",
    "# Plot the decision boundaries for the training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X=X_test, y=y_test)\n",
    "plt.show()\n",
    "\n",
    "# You can access the information in the history variable using the .history attribute\n",
    "pd.DataFrame(history.history)\n",
    "\n",
    "# Plot the loss curves\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.title(\"Model training curves\")\n",
    "\n",
    "\n",
    "model.save('models/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
